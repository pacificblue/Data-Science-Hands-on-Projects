{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\spark-2.4.6-bin-hadoop2.7'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setAppName('weather').setAll([('spark.maximizeResourceAllocation', 'true')])\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._jsc.hadoopConfiguration().setInt(\"dfs.blocksize\", 128*1024*1024)\n",
    "sc._jsc.hadoopConfiguration().setInt(\"parquet.block.size\", 1024*1024*1024)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"ScreenTemperature\", df[\"ScreenTemperature\"].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------------------+------------------+--------------------+-------+\n",
      "|summary|  ForecastSiteCode|   ObservationTime|    ObservationDate| ScreenTemperature|              Region|Country|\n",
      "+-------+------------------+------------------+-------------------+------------------+--------------------+-------+\n",
      "|  count|            194559|            194559|             194559|            194559|              194559| 166819|\n",
      "|   mean| 4740.680703539801|11.516573378769422|               null|3.3324055941899386|                null|   null|\n",
      "| stddev|11120.543403963862| 6.932137870487246|               null|13.349654981147506|                null|   null|\n",
      "|    min|              3002|                 0|2016-02-01T00:00:00|             -99.0|Central Tayside &...|ENGLAND|\n",
      "|    max|             99214|                 9|2016-03-31T00:00:00|              15.8|  Yorkshire & Humber|  WALES|\n",
      "+-------+------------------+------------------+-------------------+------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(\"ForecastSiteCode\",\"ObservationTime\", \"ObservationDate\", \"ScreenTemperature\", \"Region\", \"Country\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"ObservationDate\", df[\"ObservationDate\"].cast(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "w = Window.partitionBy([\"ObservationDate\", \"Region\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "df = df.withColumn('maxScreenTemperature', f.max('ScreenTemperature').over(w))\\\n",
    "       .where(f.col('ScreenTemperature') == f.col('maxScreenTemperature'))\\\n",
    "       .drop('maxScreenTemperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ForecastSiteCode='3023', ObservationTime='15', ObservationDate=datetime.date(2016, 2, 23), WindDirection='13', WindSpeed='6', WindGust=None, Visibility='30000', ScreenTemperature=8.1, Pressure=None, SignificantWeatherCode='7', SiteName='SOUTH UIST RANGE (3023)', Latitude='57.3580', Longitude='-7.3970', Region='Highland & Eilean Siar', Country='SCOTLAND'),\n",
       " Row(ForecastSiteCode='3148', ObservationTime='16', ObservationDate=datetime.date(2016, 3, 17), WindDirection='12', WindSpeed='5', WindGust=None, Visibility=None, ScreenTemperature=12.6, Pressure=None, SignificantWeatherCode='-99', SiteName='GLEN OGLE (3148)', Latitude='56.4230', Longitude='-4.3200', Region='Central Tayside & Fife', Country='SCOTLAND'),\n",
       " Row(ForecastSiteCode='3017', ObservationTime='10', ObservationDate=datetime.date(2016, 3, 30), WindDirection='13', WindSpeed='8', WindGust=None, Visibility='45000', ScreenTemperature=7.2, Pressure='1000', SignificantWeatherCode='7', SiteName='KIRKWALL (3017)', Latitude='58.9540', Longitude='-2.9000', Region='Orkney & Shetland', Country='SCOTLAND'),\n",
       " Row(ForecastSiteCode='3017', ObservationTime='11', ObservationDate=datetime.date(2016, 3, 30), WindDirection='13', WindSpeed='13', WindGust=None, Visibility='18000', ScreenTemperature=7.2, Pressure='1001', SignificantWeatherCode='7', SiteName='KIRKWALL (3017)', Latitude='58.9540', Longitude='-2.9000', Region='Orkney & Shetland', Country='SCOTLAND'),\n",
       " Row(ForecastSiteCode='3132', ObservationTime='20', ObservationDate=datetime.date(2016, 2, 7), WindDirection='10', WindSpeed='24', WindGust='38', Visibility=None, ScreenTemperature=7.6, Pressure='970', SignificantWeatherCode='8', SiteName='WEST FREUGH (ESAWS) (3132)', Latitude='54.8590', Longitude='-4.9360', Region='Dumfries, Galloway', Country='SCOTLAND')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.withColumn(\"partitionYear\", year(\"ObservationDate\")).withColumn(\"partitionMonth\", month(\"ObservationDate\")).withColumn(\"partitionDay\", dayofmonth(\"ObservationDate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1143"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(1).write.mode('overwrite').partitionBy(\"partitionYear\", \"partitionMonth\", \"partitionDay\").parquet(\"./result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
